PEMS07
--------- DMRCMLP ---------
{
    "num_nodes": 883,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.001,
    "milestones": [
        20,
        40,
        55
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 120,
    "early_stop": 25,
    "use_cl": false,
    "cl_step_size": 2500,
    "adaptive_mask": false,
    "change_mask_ratio": 6,
    "ratio_decay": 0.5,
    "ratio_threshold": 0.02,
    "seed": 93048409,
    "gpu": [
        4,
        5
    ],
    "save": false,
    "model_args": {
        "num_nodes": 883,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "temporal_embedding_dim": 48,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 72,
        "add_norm": false,
        "mask_ratio": 0.0,
        "use_recon": false,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_shared_layers": 2,
        "num_branch_layers": 2,
        "dropout": 0.1
    }
}
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
DMRCMLP                                       --                        888,204
├─Linear: 1-1                                 [16, 12, 883, 24]         48
├─TimeEmbedding: 1-2                          [16, 12, 883, 48]         --
│    └─Embedding: 2-1                         [16, 12, 883, 24]         6,912
│    └─Embedding: 2-2                         [16, 12, 883, 24]         168
├─ModuleList: 1-3                             --                        --
│    └─STAttnBlock: 2-3                       [16, 12, 883, 144]        --
│    │    └─SelfAttentionLayer: 3-1           [16, 12, 883, 144]        158,224
│    │    └─SelfAttentionLayer: 3-2           [16, 12, 883, 144]        158,224
│    └─STAttnBlock: 2-4                       [16, 12, 883, 144]        --
│    │    └─SelfAttentionLayer: 3-3           [16, 12, 883, 144]        158,224
│    │    └─SelfAttentionLayer: 3-4           [16, 12, 883, 144]        158,224
├─Predictor: 1-4                              [16, 12, 883, 1]          --
│    └─Linear: 2-5                            [16, 12, 883, 144]        20,880
│    └─ModuleList: 2-6                        --                        --
│    │    └─MultiLayerPerceptron: 3-5         [16, 12, 883, 144]        41,760
│    │    └─MultiLayerPerceptron: 3-6         [16, 12, 883, 144]        41,760
│    └─Linear: 2-7                            [16, 883, 12]             20,748
===============================================================================================
Total params: 1,653,376
Trainable params: 1,653,376
Non-trainable params: 0
Total mult-adds (M): 12.24
===============================================================================================
Input size (MB): 2.03
Forward/backward pass size (MB): 7932.93
Params size (MB): 3.06
Estimated Total Size (MB): 7938.02
===============================================================================================
Loss: LossFusion
Saved Model: saved_models/DMRCMLP-PEMS07-2025-01-21-05-03-02.pt
2025-01-21 05:06:34.393342 Epoch 1, Train Y Loss = 29.89148,  Train X Loss = 0.00000, Val Loss = 23.58688
2025-01-21 05:10:07.789427 Epoch 2, Train Y Loss = 23.45403,  Train X Loss = 0.00000, Val Loss = 22.26897
2025-01-21 05:13:41.455036 Epoch 3, Train Y Loss = 22.17690,  Train X Loss = 0.00000, Val Loss = 21.49711
2025-01-21 05:17:15.681303 Epoch 4, Train Y Loss = 21.26683,  Train X Loss = 0.00000, Val Loss = 21.42090
2025-01-21 05:20:50.187889 Epoch 5, Train Y Loss = 20.75670,  Train X Loss = 0.00000, Val Loss = 20.71896
2025-01-21 05:24:24.723542 Epoch 6, Train Y Loss = 20.38741,  Train X Loss = 0.00000, Val Loss = 21.04575
2025-01-21 05:27:58.240096 Epoch 7, Train Y Loss = 20.20672,  Train X Loss = 0.00000, Val Loss = 20.17050
2025-01-21 05:31:32.307636 Epoch 8, Train Y Loss = 19.83453,  Train X Loss = 0.00000, Val Loss = 20.29123
2025-01-21 05:35:05.734460 Epoch 9, Train Y Loss = 19.59110,  Train X Loss = 0.00000, Val Loss = 19.90535
2025-01-21 05:38:38.437869 Epoch 10, Train Y Loss = 19.40610,  Train X Loss = 0.00000, Val Loss = 19.45849
2025-01-21 05:42:08.815165 Epoch 11, Train Y Loss = 19.17444,  Train X Loss = 0.00000, Val Loss = 19.61655
2025-01-21 05:45:41.032431 Epoch 12, Train Y Loss = 19.01824,  Train X Loss = 0.00000, Val Loss = 19.44016
2025-01-21 05:49:12.996654 Epoch 13, Train Y Loss = 18.88826,  Train X Loss = 0.00000, Val Loss = 19.17777
2025-01-21 05:52:45.526103 Epoch 14, Train Y Loss = 18.69930,  Train X Loss = 0.00000, Val Loss = 19.12910
2025-01-21 05:56:16.130936 Epoch 15, Train Y Loss = 18.53693,  Train X Loss = 0.00000, Val Loss = 19.23220
2025-01-21 05:59:48.553048 Epoch 16, Train Y Loss = 18.46126,  Train X Loss = 0.00000, Val Loss = 18.90425
2025-01-21 06:03:19.492618 Epoch 17, Train Y Loss = 18.33233,  Train X Loss = 0.00000, Val Loss = 19.02971
2025-01-21 06:06:50.829048 Epoch 18, Train Y Loss = 18.18619,  Train X Loss = 0.00000, Val Loss = 19.00965
2025-01-21 06:10:22.841635 Epoch 19, Train Y Loss = 18.14828,  Train X Loss = 0.00000, Val Loss = 18.80906
2025-01-21 06:13:53.561253 Epoch 20, Train Y Loss = 18.03841,  Train X Loss = 0.00000, Val Loss = 19.04333
2025-01-21 06:17:24.048314 Epoch 21, Train Y Loss = 17.28973,  Train X Loss = 0.00000, Val Loss = 18.21945
2025-01-21 06:20:55.083719 Epoch 22, Train Y Loss = 17.17316,  Train X Loss = 0.00000, Val Loss = 18.21669
2025-01-21 06:24:27.769557 Epoch 23, Train Y Loss = 17.12296,  Train X Loss = 0.00000, Val Loss = 18.26936
2025-01-21 06:27:59.679937 Epoch 24, Train Y Loss = 17.07911,  Train X Loss = 0.00000, Val Loss = 18.29283
2025-01-21 06:31:32.131231 Epoch 25, Train Y Loss = 17.04417,  Train X Loss = 0.00000, Val Loss = 18.21004
2025-01-21 06:35:04.564611 Epoch 26, Train Y Loss = 17.00241,  Train X Loss = 0.00000, Val Loss = 18.18473
2025-01-21 06:38:38.060768 Epoch 27, Train Y Loss = 16.96930,  Train X Loss = 0.00000, Val Loss = 18.22360
2025-01-21 06:42:12.465309 Epoch 28, Train Y Loss = 16.93519,  Train X Loss = 0.00000, Val Loss = 18.19431
2025-01-21 06:45:46.662954 Epoch 29, Train Y Loss = 16.90172,  Train X Loss = 0.00000, Val Loss = 18.28796
2025-01-21 06:49:21.077893 Epoch 30, Train Y Loss = 16.87350,  Train X Loss = 0.00000, Val Loss = 18.25962
2025-01-21 06:52:55.276384 Epoch 31, Train Y Loss = 16.84120,  Train X Loss = 0.00000, Val Loss = 18.25147
2025-01-21 06:56:33.206411 Epoch 32, Train Y Loss = 16.81548,  Train X Loss = 0.00000, Val Loss = 18.25861
2025-01-21 07:00:10.345340 Epoch 33, Train Y Loss = 16.78114,  Train X Loss = 0.00000, Val Loss = 18.31010
2025-01-21 07:03:46.299840 Epoch 34, Train Y Loss = 16.75194,  Train X Loss = 0.00000, Val Loss = 18.29223
2025-01-21 07:07:21.711815 Epoch 35, Train Y Loss = 16.72393,  Train X Loss = 0.00000, Val Loss = 18.31464
2025-01-21 07:10:56.954079 Epoch 36, Train Y Loss = 16.69229,  Train X Loss = 0.00000, Val Loss = 18.34451
2025-01-21 07:14:32.059848 Epoch 37, Train Y Loss = 16.66747,  Train X Loss = 0.00000, Val Loss = 18.33991
2025-01-21 07:18:07.206515 Epoch 38, Train Y Loss = 16.63845,  Train X Loss = 0.00000, Val Loss = 18.35439
2025-01-21 07:21:42.060904 Epoch 39, Train Y Loss = 16.61033,  Train X Loss = 0.00000, Val Loss = 18.34370
2025-01-21 07:25:17.202224 Epoch 40, Train Y Loss = 16.58429,  Train X Loss = 0.00000, Val Loss = 18.38172
2025-01-21 07:28:52.226280 Epoch 41, Train Y Loss = 16.46540,  Train X Loss = 0.00000, Val Loss = 18.36133
2025-01-21 07:32:27.267597 Epoch 42, Train Y Loss = 16.44644,  Train X Loss = 0.00000, Val Loss = 18.36153
2025-01-21 07:36:01.890713 Epoch 43, Train Y Loss = 16.44014,  Train X Loss = 0.00000, Val Loss = 18.36565
2025-01-21 07:39:36.414598 Epoch 44, Train Y Loss = 16.43418,  Train X Loss = 0.00000, Val Loss = 18.39339
2025-01-21 07:43:10.552749 Epoch 45, Train Y Loss = 16.42812,  Train X Loss = 0.00000, Val Loss = 18.40245
2025-01-21 07:46:45.341900 Epoch 46, Train Y Loss = 16.42399,  Train X Loss = 0.00000, Val Loss = 18.40376
2025-01-21 07:50:20.017638 Epoch 47, Train Y Loss = 16.41862,  Train X Loss = 0.00000, Val Loss = 18.37998
2025-01-21 07:53:54.654861 Epoch 48, Train Y Loss = 16.41446,  Train X Loss = 0.00000, Val Loss = 18.40985
2025-01-21 07:57:29.114063 Epoch 49, Train Y Loss = 16.40962,  Train X Loss = 0.00000, Val Loss = 18.39013
2025-01-21 08:01:03.957411 Epoch 50, Train Y Loss = 16.40505,  Train X Loss = 0.00000, Val Loss = 18.41020
2025-01-21 08:04:38.346929 Epoch 51, Train Y Loss = 16.40087,  Train X Loss = 0.00000, Val Loss = 18.42001
Early stopping at epoch: 51
Best at epoch 26:
Train Loss = 17.00241
Train RMSE = 29.88973, MAE = 17.30979, MAPE = 7.59498
Val Loss = 18.18473
Val RMSE = 32.11193, MAE = 18.70691, MAPE = 8.11265
--------- Test ---------
All Steps RMSE = 32.62750, MAE = 19.16326, MAPE = 7.99065
Step 1 RMSE = 26.98375, MAE = 16.46665, MAPE = 6.90112
Step 2 RMSE = 28.89329, MAE = 17.29519, MAPE = 7.22437
Step 3 RMSE = 30.18985, MAE = 17.91907, MAPE = 7.46288
Step 4 RMSE = 31.15456, MAE = 18.40439, MAPE = 7.65484
Step 5 RMSE = 31.97538, MAE = 18.82200, MAPE = 7.87686
Step 6 RMSE = 32.68568, MAE = 19.18673, MAPE = 7.98227
Step 7 RMSE = 33.34629, MAE = 19.53564, MAPE = 8.11603
Step 8 RMSE = 33.94354, MAE = 19.86176, MAPE = 8.25215
Step 9 RMSE = 34.49417, MAE = 20.16014, MAPE = 8.38517
Step 10 RMSE = 35.01324, MAE = 20.45494, MAPE = 8.52093
Step 11 RMSE = 35.50903, MAE = 20.76200, MAPE = 8.67140
Step 12 RMSE = 36.02705, MAE = 21.08765, MAPE = 8.83848
Inference time: 28.27 s
