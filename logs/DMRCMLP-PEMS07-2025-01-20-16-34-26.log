PEMS07
--------- DMRCMLP ---------
{
    "num_nodes": 883,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.001,
    "milestones": [
        20,
        40,
        55
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 120,
    "early_stop": 25,
    "use_cl": false,
    "cl_step_size": 2500,
    "adaptive_mask": false,
    "change_mask_ratio": 6,
    "ratio_decay": 0.5,
    "ratio_threshold": 0.02,
    "seed": 51970489,
    "gpu": [
        4,
        5
    ],
    "save": false,
    "model_args": {
        "num_nodes": 883,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 1,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "temporal_embedding_dim": 48,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 72,
        "add_norm": false,
        "mask_ratio": 0.0,
        "use_recon": false,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_shared_layers": 2,
        "num_branch_layers": 2,
        "dropout": 0.1
    }
}
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
DMRCMLP                                       --                        888,204
├─Linear: 1-1                                 [16, 12, 883, 24]         48
├─TimeEmbedding: 1-2                          [16, 12, 883, 48]         --
│    └─Embedding: 2-1                         [16, 12, 883, 24]         6,912
│    └─Embedding: 2-2                         [16, 12, 883, 24]         168
├─ModuleList: 1-3                             --                        --
│    └─STAttnBlock: 2-3                       [16, 12, 883, 144]        --
│    │    └─SelfAttentionLayer: 3-1           [16, 12, 883, 144]        158,224
│    │    └─SelfAttentionLayer: 3-2           [16, 12, 883, 144]        158,224
│    └─STAttnBlock: 2-4                       [16, 12, 883, 144]        --
│    │    └─SelfAttentionLayer: 3-3           [16, 12, 883, 144]        158,224
│    │    └─SelfAttentionLayer: 3-4           [16, 12, 883, 144]        158,224
├─Predictor: 1-4                              [16, 12, 883, 1]          --
│    └─Linear: 2-5                            [16, 12, 883, 144]        20,880
│    └─ModuleList: 2-6                        --                        --
│    │    └─MultiLayerPerceptron: 3-5         [16, 12, 883, 144]        41,760
│    │    └─MultiLayerPerceptron: 3-6         [16, 12, 883, 144]        41,760
│    └─Linear: 2-7                            [16, 883, 12]             20,748
===============================================================================================
Total params: 1,653,376
Trainable params: 1,653,376
Non-trainable params: 0
Total mult-adds (M): 12.24
===============================================================================================
Input size (MB): 2.03
Forward/backward pass size (MB): 7932.93
Params size (MB): 3.06
Estimated Total Size (MB): 7938.02
===============================================================================================
Loss: LossFusion
Saved Model: saved_models/DMRCMLP-PEMS07-2025-01-20-16-34-26.pt
2025-01-20 16:37:58.319112 Epoch 1, Train Y Loss = 30.55895,  Train X Loss = 0.00000, Val Loss = 23.50115
2025-01-20 16:41:30.898166 Epoch 2, Train Y Loss = 23.70728,  Train X Loss = 0.00000, Val Loss = 22.35852
2025-01-20 16:45:03.782480 Epoch 3, Train Y Loss = 22.12617,  Train X Loss = 0.00000, Val Loss = 21.46088
2025-01-20 16:48:35.560376 Epoch 4, Train Y Loss = 21.29318,  Train X Loss = 0.00000, Val Loss = 21.07048
2025-01-20 16:52:06.571977 Epoch 5, Train Y Loss = 20.70294,  Train X Loss = 0.00000, Val Loss = 20.73292
2025-01-20 16:55:37.523983 Epoch 6, Train Y Loss = 20.33957,  Train X Loss = 0.00000, Val Loss = 20.36533
2025-01-20 16:59:08.100901 Epoch 7, Train Y Loss = 19.98312,  Train X Loss = 0.00000, Val Loss = 19.90204
2025-01-20 17:02:38.933432 Epoch 8, Train Y Loss = 19.68568,  Train X Loss = 0.00000, Val Loss = 19.74480
2025-01-20 17:06:09.298471 Epoch 9, Train Y Loss = 19.34720,  Train X Loss = 0.00000, Val Loss = 20.37196
2025-01-20 17:09:39.836280 Epoch 10, Train Y Loss = 19.15927,  Train X Loss = 0.00000, Val Loss = 19.78247
2025-01-20 17:13:10.254538 Epoch 11, Train Y Loss = 18.98312,  Train X Loss = 0.00000, Val Loss = 19.29358
2025-01-20 17:16:40.550691 Epoch 12, Train Y Loss = 18.83529,  Train X Loss = 0.00000, Val Loss = 19.12457
2025-01-20 17:20:10.893781 Epoch 13, Train Y Loss = 18.69243,  Train X Loss = 0.00000, Val Loss = 19.16715
2025-01-20 17:23:41.730443 Epoch 14, Train Y Loss = 18.57875,  Train X Loss = 0.00000, Val Loss = 19.19201
2025-01-20 17:27:13.128633 Epoch 15, Train Y Loss = 18.40803,  Train X Loss = 0.00000, Val Loss = 18.89014
2025-01-20 17:30:43.461249 Epoch 16, Train Y Loss = 18.33833,  Train X Loss = 0.00000, Val Loss = 18.96483
2025-01-20 17:34:13.861635 Epoch 17, Train Y Loss = 18.24303,  Train X Loss = 0.00000, Val Loss = 18.85123
2025-01-20 17:37:44.582609 Epoch 18, Train Y Loss = 18.13682,  Train X Loss = 0.00000, Val Loss = 18.84255
2025-01-20 17:41:15.946465 Epoch 19, Train Y Loss = 18.07622,  Train X Loss = 0.00000, Val Loss = 18.88618
2025-01-20 17:44:47.224349 Epoch 20, Train Y Loss = 17.97662,  Train X Loss = 0.00000, Val Loss = 18.88196
2025-01-20 17:48:18.779405 Epoch 21, Train Y Loss = 17.27160,  Train X Loss = 0.00000, Val Loss = 18.24469
2025-01-20 17:51:50.312938 Epoch 22, Train Y Loss = 17.15725,  Train X Loss = 0.00000, Val Loss = 18.25620
2025-01-20 17:55:21.942030 Epoch 23, Train Y Loss = 17.11193,  Train X Loss = 0.00000, Val Loss = 18.24014
2025-01-20 17:58:53.676574 Epoch 24, Train Y Loss = 17.07327,  Train X Loss = 0.00000, Val Loss = 18.25443
2025-01-20 18:02:25.110568 Epoch 25, Train Y Loss = 17.04146,  Train X Loss = 0.00000, Val Loss = 18.22353
2025-01-20 18:05:57.077092 Epoch 26, Train Y Loss = 17.00866,  Train X Loss = 0.00000, Val Loss = 18.24918
2025-01-20 18:09:28.110589 Epoch 27, Train Y Loss = 16.98173,  Train X Loss = 0.00000, Val Loss = 18.25447
2025-01-20 18:12:58.928930 Epoch 28, Train Y Loss = 16.95478,  Train X Loss = 0.00000, Val Loss = 18.26282
2025-01-20 18:16:30.306584 Epoch 29, Train Y Loss = 16.92934,  Train X Loss = 0.00000, Val Loss = 18.26923
2025-01-20 18:20:01.024743 Epoch 30, Train Y Loss = 16.90742,  Train X Loss = 0.00000, Val Loss = 18.24204
2025-01-20 18:23:32.410433 Epoch 31, Train Y Loss = 16.87899,  Train X Loss = 0.00000, Val Loss = 18.29089
2025-01-20 18:27:03.936035 Epoch 32, Train Y Loss = 16.85526,  Train X Loss = 0.00000, Val Loss = 18.26702
2025-01-20 18:30:35.170962 Epoch 33, Train Y Loss = 16.83328,  Train X Loss = 0.00000, Val Loss = 18.29886
2025-01-20 18:34:05.965500 Epoch 34, Train Y Loss = 16.80871,  Train X Loss = 0.00000, Val Loss = 18.30219
2025-01-20 18:37:36.704288 Epoch 35, Train Y Loss = 16.78957,  Train X Loss = 0.00000, Val Loss = 18.29478
2025-01-20 18:41:07.400601 Epoch 36, Train Y Loss = 16.76503,  Train X Loss = 0.00000, Val Loss = 18.29943
2025-01-20 18:44:38.642276 Epoch 37, Train Y Loss = 16.74628,  Train X Loss = 0.00000, Val Loss = 18.30541
2025-01-20 18:48:09.810692 Epoch 38, Train Y Loss = 16.72184,  Train X Loss = 0.00000, Val Loss = 18.38997
2025-01-20 18:51:42.068536 Epoch 39, Train Y Loss = 16.70239,  Train X Loss = 0.00000, Val Loss = 18.32132
2025-01-20 18:55:13.936303 Epoch 40, Train Y Loss = 16.68245,  Train X Loss = 0.00000, Val Loss = 18.29043
2025-01-20 18:58:45.712461 Epoch 41, Train Y Loss = 16.57289,  Train X Loss = 0.00000, Val Loss = 18.27682
2025-01-20 19:02:17.785421 Epoch 42, Train Y Loss = 16.56005,  Train X Loss = 0.00000, Val Loss = 18.28001
2025-01-20 19:05:49.697393 Epoch 43, Train Y Loss = 16.55406,  Train X Loss = 0.00000, Val Loss = 18.27219
2025-01-20 19:09:21.292446 Epoch 44, Train Y Loss = 16.54791,  Train X Loss = 0.00000, Val Loss = 18.28869
2025-01-20 19:12:53.005790 Epoch 45, Train Y Loss = 16.54460,  Train X Loss = 0.00000, Val Loss = 18.27948
2025-01-20 19:16:24.291465 Epoch 46, Train Y Loss = 16.54054,  Train X Loss = 0.00000, Val Loss = 18.29885
2025-01-20 19:19:56.155222 Epoch 47, Train Y Loss = 16.53697,  Train X Loss = 0.00000, Val Loss = 18.30985
2025-01-20 19:23:28.119961 Epoch 48, Train Y Loss = 16.53399,  Train X Loss = 0.00000, Val Loss = 18.30012
2025-01-20 19:27:00.338990 Epoch 49, Train Y Loss = 16.53020,  Train X Loss = 0.00000, Val Loss = 18.29960
2025-01-20 19:30:32.268314 Epoch 50, Train Y Loss = 16.52649,  Train X Loss = 0.00000, Val Loss = 18.31604
Early stopping at epoch: 50
Best at epoch 25:
Train Loss = 17.04146
Train RMSE = 29.90722, MAE = 17.36761, MAPE = 7.61323
Val Loss = 18.22353
Val RMSE = 32.21152, MAE = 18.74509, MAPE = 8.17021
--------- Test ---------
All Steps RMSE = 32.79260, MAE = 19.20576, MAPE = 8.02407
Step 1 RMSE = 26.95585, MAE = 16.44334, MAPE = 6.92241
Step 2 RMSE = 28.93277, MAE = 17.28482, MAPE = 7.23095
Step 3 RMSE = 30.25840, MAE = 17.92908, MAPE = 7.51489
Step 4 RMSE = 31.28713, MAE = 18.41703, MAPE = 7.69801
Step 5 RMSE = 32.17027, MAE = 18.86222, MAPE = 7.88072
Step 6 RMSE = 32.89306, MAE = 19.22653, MAPE = 8.00402
Step 7 RMSE = 33.54827, MAE = 19.57890, MAPE = 8.16216
Step 8 RMSE = 34.12719, MAE = 19.89030, MAPE = 8.29350
Step 9 RMSE = 34.67549, MAE = 20.19295, MAPE = 8.41107
Step 10 RMSE = 35.18959, MAE = 20.48583, MAPE = 8.55217
Step 11 RMSE = 35.75519, MAE = 20.88833, MAPE = 8.72491
Step 12 RMSE = 36.32860, MAE = 21.26732, MAPE = 8.89287
Inference time: 27.50 s
